{
  
    
        "post0": {
            "title": "1. Why Data Cleaning and Preprocessing is Important ?",
            "content": "Data Cleaning and preprocessing is the most critical step in any Data science project . Data cleaning is the process of transforming row dataset into understandable format.Real world data is often incomplete,inaccurate , inconsistent and noisy. . There is no doubt that datascientist job is the &quot;sexiest job of the 21&#39;st century&quot;, But what data scientists spend the most time doing?? Most data scientists spend only 20 percent of their time on actual data modelling and 80 percent of their time finding, cleaning, and reorganizing huge amounts of data. . . In this blog post, I am going to clean and prepare the dataset. And by saying that I mean , i am going to transform dataset from messy to tidy and make it useful for machine learning models. . Let me tell you what I will cover - . Why Data Cleaning and Preprocessing is Important | About Dataset | Handling Missing values | Handling Outliers | Encoding categorical features | Feature scaling | Feature Selection and Feature Engineering | So , let&#39;s get started . Before we start discussing data cleaning and preprocessing and it&#39;s importance. Let&#39;s first discuss about Datascience pipeline. . . Acquire : This step involves the proper understanding of the question we are trying to answer and having enough knowledge about the data. . Cleanse : Cleansing involves understanding dataquality issues like unnecessary data, duplication, resolving missing values etc.This is an essential step in the pipeline as the machine learning models’ accuracy heavily depends on the data. . Explore : This part involves understanding the data and patterns before going on to build a model. This step is used to understand the importance of different variables, the relationship between different variables, understanding the data distributions, and forming hypothesis. . Model : This part involves creating statistical or machine learning models for prediction. The outcome of this process is making inferences, predicting a future event, or providing the “cause” for an event. . Present : The findings from the model need to be presented, in most cases, to a non-technical audience. This is in the form of summary writeups, charts, PowerPoints, etc. . Training ML algorithms and utilizing them to predict the target variable is the easy,because of various libraries and packages available in Python and R. Collecting data and cleaning it can be a lot of work, I don’t think this part particularly hard. It’s definitely important and may require careful planning. . Why is it Important ? . When combining multiple data sources, there are many opportunities for data to be duplicated or mislabeled. If data is incorrect, outcomes from algorithms will also be incorrrect , even though they may look correct.This step is crucial because wrong data can drive a business to wrong decisions, conclusions, and poor analysis, especially if the huge quantities of big data are into the picture. . . 2. About Dataset . Zillow’s Home Value Prediction (Zestimate) . Zillow’s Zestimate home valuation has shaken up the U.S. real estate industry since first released 11 years ago. . Zillow Prize, a competition with a one million dollar grand prize ( 3 years ago ), participants was required to develop an algorithm that makes predictions about the future sale prices of homes. . I am going to clean and prepare this dataset. . For more information about the dataset you can refer - https://www.kaggle.com/c/zillow-prize-1 . import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns import missingno as msno from sklearn import linear_model from sklearn.impute import KNNImputer from sklearn.preprocessing import OneHotEncoder,StandardScaler,LabelEncoder pd.set_option(&quot;display.max_columns&quot;, 100) . properties = pd.read_csv(&#39;/content/properties_2017.csv&#39;) for c, dtype in zip(properties.columns, properties.dtypes): if dtype == np.float64: properties[c] = properties[c].astype(np.float32) . properties.head() . parcelid airconditioningtypeid architecturalstyletypeid basementsqft bathroomcnt bedroomcnt buildingclasstypeid buildingqualitytypeid calculatedbathnbr decktypeid finishedfloor1squarefeet calculatedfinishedsquarefeet finishedsquarefeet12 finishedsquarefeet13 finishedsquarefeet15 finishedsquarefeet50 finishedsquarefeet6 fips fireplacecnt fullbathcnt garagecarcnt garagetotalsqft hashottuborspa heatingorsystemtypeid latitude longitude lotsizesquarefeet poolcnt poolsizesum pooltypeid10 pooltypeid2 pooltypeid7 propertycountylandusecode propertylandusetypeid propertyzoningdesc rawcensustractandblock regionidcity regionidcounty regionidneighborhood regionidzip roomcnt storytypeid threequarterbathnbr typeconstructiontypeid unitcnt yardbuildingsqft17 yardbuildingsqft26 yearbuilt numberofstories fireplaceflag structuretaxvaluedollarcnt taxvaluedollarcnt assessmentyear landtaxvaluedollarcnt taxamount taxdelinquencyflag taxdelinquencyyear censustractandblock . 0 10754147 | NaN | NaN | NaN | 0.0 | 0.0 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | 6037 | NaN | NaN | NaN | NaN | NaN | NaN | 34144442 | -118654084 | 85768.0 | NaN | NaN | NaN | NaN | NaN | 010D | 269 | NaN | 60378004.0 | 37688.0 | 3101.0 | NaN | 96337.0 | 0.0 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | 9.0 | 2016.0 | 9.0 | NaN | NaN | NaN | NaN | . 1 10759547 | NaN | NaN | NaN | 0.0 | 0.0 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | 6037 | NaN | NaN | NaN | NaN | NaN | NaN | 34140430 | -118625364 | 4083.0 | NaN | NaN | NaN | NaN | NaN | 0109 | 261 | LCA11* | 60378000.0 | 37688.0 | 3101.0 | NaN | 96337.0 | 0.0 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | 27516.0 | 2015.0 | 27516.0 | NaN | NaN | NaN | NaN | . 2 10843547 | NaN | NaN | NaN | 0.0 | 0.0 | 5.0 | NaN | NaN | NaN | NaN | 73026.0 | NaN | NaN | 73026.0 | NaN | NaN | 6037 | NaN | NaN | NaN | NaN | NaN | NaN | 33989359 | -118394633 | 63085.0 | NaN | NaN | NaN | NaN | NaN | 1200 | 47 | LAC2 | 60377032.0 | 51617.0 | 3101.0 | NaN | 96095.0 | 0.0 | NaN | NaN | NaN | 2.0 | NaN | NaN | 1959.0 | 1.0 | NaN | 660680.0 | 1434941.0 | 2016.0 | 774261.0 | 20800.369141 | NaN | NaN | NaN | . 3 10859147 | NaN | NaN | NaN | 0.0 | 0.0 | 3.0 | 6.0 | NaN | NaN | NaN | 5068.0 | NaN | NaN | 5068.0 | NaN | NaN | 6037 | NaN | NaN | NaN | NaN | NaN | NaN | 34148863 | -118437206 | 7521.0 | NaN | NaN | NaN | NaN | NaN | 1200 | 47 | LAC2 | 60371412.0 | 12447.0 | 3101.0 | 27080.0 | 96424.0 | 0.0 | NaN | NaN | NaN | NaN | NaN | NaN | 1948.0 | 1.0 | NaN | 580059.0 | 1174475.0 | 2016.0 | 594416.0 | 14557.570312 | NaN | NaN | NaN | . 4 10879947 | NaN | NaN | NaN | 0.0 | 0.0 | 4.0 | NaN | NaN | NaN | NaN | 1776.0 | NaN | NaN | 1776.0 | NaN | NaN | 6037 | NaN | NaN | NaN | NaN | NaN | NaN | 34194168 | -118385816 | 8512.0 | NaN | NaN | NaN | NaN | NaN | 1210 | 31 | LAM1 | 60371232.0 | 12447.0 | 3101.0 | 46795.0 | 96450.0 | 0.0 | NaN | NaN | NaN | 1.0 | NaN | NaN | 1947.0 | 1.0 | NaN | 196751.0 | 440101.0 | 2016.0 | 243350.0 | 5725.169922 | NaN | NaN | NaN | . 3. Handling Missing values in Dataset . . 3.1 Types of Missing values . . MCAR(Missing Completely at Random) means there is no relationship between the missingness of the data and any values, observed or missing. In other words, there is no particular reason for the missing values. . MAR(Missing At Random) MAR occurs when the missingness is not random, but where missingness can be fully accounted for by variables where there is complete information. . MNAR(Missing Not at Random) - Missingness depends on unobserved data or the value of the missing data itself. . 3.2 Detecting Missing values . missing_df = properties.isnull().sum(axis=0).reset_index() missing_df.columns = [&#39;column&#39;, &#39;missing_count&#39;] missing_df = missing_df.loc[missing_df[&#39;missing_count&#39;]&gt;0] missing_df = missing_df.sort_values(by=&#39;missing_count&#39;) ind = np.arange(missing_df.shape[0]) fig, ax = plt.subplots(figsize=(12,16)) rects = ax.barh(ind, missing_df.missing_count.values, color=&#39;red&#39;) ax.set_yticks(ind) ax.set_yticklabels(missing_df.column.values, rotation=&#39;horizontal&#39;) ax.set_xlabel(&quot;Count of missing values&quot;) ax.set_title(&quot;Number of missing values in each column&quot;) plt.show() . . 3.3 Visualising relationship between columns using HeatMap . categorical_cols = [&#39;airconditioningtypeid&#39;,&#39;architecturalstyletypeid&#39;,&#39;buildingqualitytypeid&#39;,&#39;buildingclasstypeid&#39;,&#39;decktypeid&#39;,&#39;fips&#39;, &#39;hashottuborspa&#39;,&#39;heatingorsystemtypeid&#39;,&#39;pooltypeid10&#39;,&#39;pooltypeid2&#39;,&#39;pooltypeid7&#39;,&#39;propertycountylandusecode&#39;, &#39;propertylandusetypeid&#39;,&#39;propertyzoningdesc&#39;,&#39;rawcensustractandblock&#39;,&#39;regionidcity&#39;,&#39;regionidcounty&#39;,&#39;regionidneighborhood&#39;, &#39;regionidzip&#39;,&#39;storytypeid&#39;,&#39;typeconstructiontypeid&#39;,&#39;yearbuilt&#39;,&#39;taxdelinquencyflag&#39;,&#39;fireplaceflag&#39;] numerical_cols = [x for x in properties.columns if x not in categorical_cols] #Lets start by plotting a heatmap to determine if any variables are correlated plt.figure(figsize = (12,8)) sns.heatmap(data=properties[numerical_cols].corr()) plt.show() plt.gcf().clear() . &lt;Figure size 432x288 with 0 Axes&gt; . calculatedfinishedsquarefeet&#39; , &#39;finishedsquarefeet12&#39; , &#39;finishedsquarefeet13&#39; , &#39;finishedsquarefeet15&#39; , &#39;finishedsquarefeet6&#39; , &#39;finishedsquarefeet50&#39; are strongly related and they represent very similair pieces of information (the area of the property) . lets pick the one with the fewest number of missing values and drop the rest . Note: I am creating dropcolumn list which will contain all the columns to be deleted. . dropcolumns = [&#39;finishedsquarefeet12&#39;,&#39;finishedsquarefeet13&#39;, &#39;finishedsquarefeet15&#39;,&#39;finishedsquarefeet6&#39;,&#39;finishedsquarefeet50&#39;] #&#39;bathroomcnt&#39; and &#39;calculatedbathnbr&#39; and &#39;fullbathcnt&#39; represent same information as well. Choose &#39;bathroomcnt&#39; as has no missing values, so remove the other two. dropcolumns.append(&#39;calculatedbathnbr&#39;) dropcolumns.append(&#39;fullbathcnt&#39;) . 3.4 Basic Imputation Methods . 1. Pairwise Deletion . Parwise Deletion is used when values are missing completely at random i.e MCAR. During Pairwise deletion, only the missing values are deleted. . 2. Listwise Deletion/ Dropping rows . During Listwise deletion, complete rows(which contain the missing values) are deleted. As a result, it is also called Complete Case deletion. Like Pairwise deletion, listwise deletions are also only used for MCAR . . 3. Dropping complete columns . If a column contains a lot of missing values, say more than 90%(some threshold), you might want to delete that feature. However, This is however not recommended as it might result in loss of information from the dataset. . 4. Deductive Imputation . Sometimes, from the pattern of responses of the other items, the missing responses to an item can be deduced with certainty.For example , university faculty member may be considered to have degree holder. . Disadvantage to this approach is that the occasions when it can be applied are quite rare. . 5. Constant Imputation . Imputating with a constant value | Imputation using the statistics (mean, median or most frequent) of the missing value column. | . why not impute the missing data with the Measure of Central Tendency of the variable? . When we replace the missing data with some common value we might under(over)estimate it. In other words, we are adding some bias to our estimation. For example, a person who earns just enough to meet his daily needs might not be comfortable in mentioning his salary, and thus the value for the variable salary would be missing for such a person. However, if we impute it with the mean value of the variable, we are overestimating that person&#39;s salary and thus introducing bias in our analysis. . Let&#39;s see how we can use following methods in our dataset . &#39;pooltypeid10&#39; - Spa or Hot Tub . | &#39;pooltypeid2&#39;- Pool with Spa/Hot Tub . | &#39;pooltypeid7&#39; - Pool without hot tub . | &#39;hashottuborspa&#39; - Does the home have a hot tub or spa . | poolcnt - Number of pools on the lot (if any) . | . We can remove one of the pooltypeid10 and pooltypeid2, as it convey same information, removing pooltypeid10 as it has more missing values . dropcolumns.append(&#39;pooltypeid10&#39;) . replace &#39;NA&#39; with None as NA here means pool doesnot exists . properties[&#39;hashottuborspa&#39;]=properties[&#39;hashottuborspa&#39;].fillna(&quot;None&quot;) properties[&#39;pooltypeid2&#39;]=properties[&#39;pooltypeid2&#39;].fillna(&quot;None&quot;) . Replace &#39;pooltypeid7&#39; NA with 0 as 1 shows &#39;Pool without hot tub&#39; so 0 will show &#39;Pool with hot tub&#39; . properties[&quot;pooltypeid7&quot;]=properties[&quot;pooltypeid7&quot;].fillna(0) . Replace &#39;poolcnt&#39; NA with 0 (No pool) . properties[&#39;poolcnt&#39;] = properties[&#39;poolcnt&#39;].fillna(0) . Fill in those properties that have a pool with median pool value . poolsizesum_median = properties.loc[properties[&#39;poolcnt&#39;] &gt; 0, &#39;poolsizesum&#39;].median() properties.loc[(properties[&#39;poolcnt&#39;] &gt; 0) &amp; (properties[&#39;poolsizesum&#39;].isnull()), &#39;poolsizesum&#39;] = poolsizesum_median . If it doesn&#39;t have a pool then poolsizesum is 0 by default . properties.loc[(properties[&#39;poolcnt&#39;] == 0), &#39;poolsizesum&#39;] = 0 . Missing values in fireplaceflag and fireplacecnt should be same , but it is not, so I am changing fireplaceflag to &quot;No&quot; when fireplacecnt is 0 otherwise &quot;Yes&quot;. . properties[&#39;fireplaceflag&#39;].isnull().sum(),properties[&#39;fireplacecnt&#39;].isnull().sum() . (1258426, 1128219) . properties[&#39;fireplaceflag&#39;]= &quot;No&quot; properties.loc[properties[&#39;fireplacecnt&#39;]&gt;0,&#39;fireplaceflag&#39;]= &quot;Yes&quot; . properties[&#39;fireplacecnt&#39;]=properties[&#39;fireplacecnt&#39;].fillna(0) . Tax deliquency flag - if it is null then doesn&#39;t exist . properties[&#39;taxdelinquencyflag&#39;]=properties[&#39;taxdelinquencyflag&#39;].fillna(&quot;None&quot;) . garagecarcnt,garagetotalsqft has same number of missing values , when there are properties with no garages then both variables are NA , so filling missing values of both variables with 0. . print(properties[&#39;garagecarcnt&#39;].isna().sum(),properties[&#39;garagetotalsqft&#39;].isna().sum()) properties[&#39;garagecarcnt&#39;]=properties[&#39;garagecarcnt&#39;].fillna(0) properties[&#39;garagetotalsqft&#39;]=properties[&#39;garagetotalsqft&#39;].fillna(0) . 885255 885255 . Airconditioningtypeid - Type of cooling system present in the home (if any) . So ,I am replace NA with 0 ( representing None ) . properties[&#39;airconditioningtypeid&#39;].value_counts() . 1.0 314660 13.0 24559 5.0 3676 9.0 838 11.0 775 12.0 26 3.0 3 Name: airconditioningtypeid, dtype: int64 . properties[&#39;airconditioningtypeid&#39;]=properties[&#39;airconditioningtypeid&#39;].fillna(0) . heatingorsystemtypeid - Type of home heating system . Replacing NA with 0 . properties[&#39;heatingorsystemtypeid&#39;].value_counts() . 2.0 494229 7.0 273909 6.0 11537 24.0 8950 20.0 1586 13.0 558 18.0 242 1.0 102 14.0 14 10.0 12 12.0 11 11.0 9 21.0 2 Name: heatingorsystemtypeid, dtype: int64 . properties[&#39;heatingorsystemtypeid&#39;]=properties[&#39;heatingorsystemtypeid&#39;].fillna(0) . Filling Missing in threequarterbathnbr column with 0 . properties[&#39;threequarterbathnbr&#39;] = properties[&#39;threequarterbathnbr&#39;].fillna(0) . There&#39;s still fields with alot of data missing. Let&#39;s remove fields that have 97% missing values. . def get_missing_table(df): missingvalues_perc = (df.isnull().sum()/len(df)*100).reset_index() missingvalues_perc.columns = [&#39;field&#39;,&#39;percentage&#39;] missingvalues_perc = missingvalues_perc.sort_values(by = &#39;percentage&#39;, ascending = False) return missingvalues_perc . missing_values_table=get_missing_table(properties) missing_values_cols = missing_values_table[missing_values_table[&#39;percentage&#39;] &gt; 97.0].field.tolist() dropcolumns = dropcolumns + missing_values_cols . properties.drop(dropcolumns,axis=1,inplace=True) . get_missing_table(properties) . field percentage . 5 finishedfloor1squarefeet | 93.134687 | . 32 numberofstories | 77.072151 | . 26 regionidneighborhood | 61.145504 | . 4 buildingqualitytypeid | 34.795549 | . 30 unitcnt | 33.465466 | . 22 propertyzoningdesc | 33.425247 | . 15 lotsizesquarefeet | 9.017546 | . 40 censustractandblock | 2.423056 | . 24 regionidcity | 1.973984 | . 37 landtaxvaluedollarcnt | 1.925039 | . 31 yearbuilt | 1.494925 | . 34 structuretaxvaluedollarcnt | 1.452009 | . 6 calculatedfinishedsquarefeet | 1.403382 | . 35 taxvaluedollarcnt | 1.059894 | . 38 taxamount | 0.658021 | . 27 regionidzip | 0.324608 | . 20 propertycountylandusecode | 0.002459 | . 28 roomcnt | 0.000873 | . 2 bathroomcnt | 0.000635 | . 3 bedroomcnt | 0.000238 | . 36 assessmentyear | 0.000079 | . 25 regionidcounty | 0.000079 | . 17 poolsizesum | 0.000000 | . 11 hashottuborspa | 0.000000 | . 39 taxdelinquencyflag | 0.000000 | . 7 fips | 0.000000 | . 8 fireplacecnt | 0.000000 | . 33 fireplaceflag | 0.000000 | . 9 garagecarcnt | 0.000000 | . 10 garagetotalsqft | 0.000000 | . 29 threequarterbathnbr | 0.000000 | . 18 pooltypeid2 | 0.000000 | . 12 heatingorsystemtypeid | 0.000000 | . 13 latitude | 0.000000 | . 14 longitude | 0.000000 | . 23 rawcensustractandblock | 0.000000 | . 16 poolcnt | 0.000000 | . 21 propertylandusetypeid | 0.000000 | . 1 airconditioningtypeid | 0.000000 | . 19 pooltypeid7 | 0.000000 | . 0 parcelid | 0.000000 | . 3.5 Advance Imputation Methods . 1. Regression Imputation . . Regression Imputation is method in which we estimate the missing values by Regression using other variables as the parameters. . numerical_cols = [x for x in properties.columns if x not in categorical_cols] categorical_cols = [x for x in properties.columns if x not in numerical_cols] . Lets fill tax related columns missing values using Regression. . So there are four tax related columns - . taxvaluedollarcnt - The total tax assessed value of the parcel | landtaxvaluedollarcnt - The assessed value of the land area of the parcel | taxamount - The total property tax assessed for that assessment year. | structuretaxvaluedollarcnt - The assessed value of the built structure on the parcel | properties[[&#39;taxvaluedollarcnt&#39;,&#39;landtaxvaluedollarcnt&#39;,&#39;taxamount&#39;,&#39;structuretaxvaluedollarcnt&#39;]].corr() . taxvaluedollarcnt landtaxvaluedollarcnt taxamount structuretaxvaluedollarcnt . taxvaluedollarcnt 1.000000 | 0.844842 | 0.985945 | 0.885595 | . landtaxvaluedollarcnt 0.844842 | 1.000000 | 0.808594 | 0.500127 | . taxamount 0.985945 | 0.808594 | 1.000000 | 0.887049 | . structuretaxvaluedollarcnt 0.885595 | 0.500127 | 0.887049 | 1.000000 | . taxamount and taxvaluedollarcnt are highly correlated (0.97), so we will delete one of them. . I am keeping taxamount as it has less missing values. . properties.drop([&#39;taxvaluedollarcnt&#39;],axis=1,inplace=True) . numerical_cols=numerical_cols.remove(&#39;taxvaluedollarcnt&#39;) . Initially imputing all the variables with missing values using some trivial methods like Simple Random Imputation (we impute the missing data with random observed values of the variable) which is later followed by Regression Imputation of each of the variables iteratively. . missing_columns_regr=[&#39;landtaxvaluedollarcnt&#39;,&#39;taxamount&#39;,&#39;structuretaxvaluedollarcnt&#39;] . The function random_imputation replaces the missing values with some random observed values of the variable. The method is repeated for all the variables containing missing values, after which they serve as parameters in the regression model to estimate other variable values. . Simple Random Imputation is one of the crude methods since it ignores all the other available data and thus it&#39;s very rarely used. But it serves as a good starting point for regression imputation. . def random_imputation(df, feature): number_missing = df[feature].isnull().sum() observed_values = df.loc[df[feature].notnull(), feature] df.loc[df[feature].isnull(), feature + &#39;_imp&#39;] = np.random.choice(observed_values, number_missing, replace = True) return df . for feature in missing_columns_regr: properties[feature + &#39;_imp&#39;] = properties[feature] properties = random_imputation(properties, feature) . Now we replace the missing data with the values predicted in our regression model and repeat this process for each variable. . non_missing_values_columns=properties.columns[~properties.isnull().any()] non_missing_num_columns=list(set(non_missing_values_columns)-set(categorical_cols)) . deter_data = pd.DataFrame(columns = [&quot;Det&quot; + name for name in missing_columns_regr]) for feature in missing_columns_regr: deter_data[&quot;Det&quot; + feature] = properties[feature + &quot;_imp&quot;] parameters = list(set(non_missing_num_columns) - {feature + &#39;_imp&#39;}) #Create a Linear Regression model to estimate the missing data model = linear_model.LinearRegression() model.fit(X = properties[parameters], y = properties[feature + &#39;_imp&#39;]) #observe that I preserve the index of the missing data from the original dataframe deter_data.loc[properties[feature].isnull(), &quot;Det&quot; + feature] = model.predict(properties[parameters])[properties[feature].isnull()] . properties[missing_columns_regr]=deter_data[[&quot;Detlandtaxvaluedollarcnt&quot;, &quot;Dettaxamount&quot;,&quot;Detstructuretaxvaluedollarcnt&quot;]] properties.drop([&#39;landtaxvaluedollarcnt_imp&#39;,&#39;taxamount_imp&#39;,&#39;structuretaxvaluedollarcnt_imp&#39;],axis=1,inplace=True) . 2. K Nearest Neighbour (KNN) Imputation . KNN used to partition the objects which are similar within groups and are dissimilar with each other.k-nn, is an approach that estimates how likely a data point is to be a member of one group or the other depending on what group the data points nearest to it are in. . . function to impute missing values using KNN . def impute_knn(df,target): imputer = KNNImputer(n_neighbors=1) res=imputer.fit_transform(df[[&#39;longitude&#39;,&#39;latitude&#39;,target]],y=df[target]) df_knn_imputed=pd.DataFrame(res,columns=[&#39;longitude&#39;,&#39;latitude&#39;,target]) return df_knn_imputed . function to deal with variables that are actually string/categories . def zoningcode2int( df, target ): storenull = df[target ].isnull() enc = LabelEncoder( ) df[ target ] = df[target ].astype( str ) df[target ]= enc.fit_transform( df[ target ].values ) print( &#39;num of categories: &#39;, enc.classes_.shape ) df.loc[ storenull, target ] = np.nan return enc . Impute regionidcity . df_knn_imputed=impute_knn(properties,&#39;regionidcity&#39;) properties[&#39;regionidcity&#39;]=df_knn_imputed[&#39;regionidcity&#39;] . Impute propertyzoningdesc . zoningcode2int(properties,&#39;propertyzoningdesc&#39;) df_knn_imputed=impute_knn(properties,&#39;propertyzoningdesc&#39;) properties[&#39;propertyzoningdesc&#39;]=df_knn_imputed[&#39;propertyzoningdesc&#39;] . Impute propertycountylandusecode . zoningcode2int(properties,&#39;propertycountylandusecode&#39;) df_knn_imputed=impute_knn(properties,&#39;propertycountylandusecode&#39;) properties[&#39;propertycountylandusecode&#39;]=df_knn_imputed[&#39;propertycountylandusecode&#39;] . Impute regionidneighborhood . df_knn_imputed=impute_knn(properties,&#39;regionidneighborhood&#39;) properties[&#39;regionidneighborhood&#39;]=df_knn_imputed[&#39;regionidneighborhood&#39;] . Impute regionidzip . df_knn_imputed=impute_knn(properties,&#39;regionidzip&#39;) properties[&#39;regionidzip&#39;]=df_knn_imputed[&#39;regionidzip&#39;] . Impute unitcnt . df_knn_imputed=impute_knn(properties,&#39;unitcnt&#39;) properties[&#39;unitcnt&#39;]=df_knn_imputed[&#39;unitcnt&#39;] . Impute yearbuilt . df_knn_imputed=impute_knn(properties,&#39;yearbuilt&#39;) properties[&#39;yearbuilt&#39;]=df_knn_imputed[&#39;yearbuilt&#39;] . Impute lotsizesquarefeet . df_knn_imputed=impute_knn(properties,&#39;lotsizesquarefeet&#39;) properties[&#39;lotsizesquarefeet&#39;]=df_knn_imputed[&#39;lotsizesquarefeet&#39;] .",
            "url": "https://yashvip.github.io/convert_to_blog/2020/11/18/data_cleaning-(1).html",
            "relUrl": "/2020/11/18/data_cleaning-(1).html",
            "date": " • Nov 18, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://yashvip.github.io/convert_to_blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://yashvip.github.io/convert_to_blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://yashvip.github.io/convert_to_blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://yashvip.github.io/convert_to_blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}